{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d1a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f451ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "  RecursiveCharacterTextSplitter ,\n",
    "  MarkdownHeaderTextSplitter\n",
    "\n",
    ")\n",
    "\n",
    "sample_text = \"\"\"\n",
    "# Intro\n",
    "LangChain helps build LLM apps. Chunking is step one.\n",
    "\n",
    "## What is chunking?\n",
    "We split long docs into smaller pieces, often with overlap, to help retrieval.\n",
    "\n",
    "### Example\n",
    "This paragraph should stay near its header when possible.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca209f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recursion 1 chunks\n",
      "0 233 1\n"
     ]
    }
   ],
   "source": [
    "#recursive chunking \n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "rec_chunks=rec_splitter.create_documents([sample_text])\n",
    "\n",
    "print(\"Recursion\",len(rec_chunks),\"chunks\")\n",
    "\n",
    "for i, c in enumerate(rec_chunks[:2]):\n",
    "    print(i, len(c.page_content), c.metadata.get(\"start_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1433837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# markdown aware (keep headers)\n",
    "\n",
    "headers=[('#','h1'),(\"##\",'h2'),('###','h3')]\n",
    "\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)\n",
    "\n",
    "md_docs=md_splitter.split_text(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dfec8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown-aware 3 chunks\n",
      "0 {'h1': 'Intro', 'start_index': 0}\n",
      "1 {'h1': 'Intro', 'h2': 'What is chunking?', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "md_chunks=rec_splitter.split_documents(md_docs)\n",
    "\n",
    "print('markdown-aware',len(md_chunks),'chunks')\n",
    "\n",
    "for i,c in enumerate(md_chunks[:2]):\n",
    "    print(i,c.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c806d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: Dogs are loyal animals. They love playing fetch.\n",
      "Chunk 1: Quantum mechanics studies subatomic particles. It is very complex.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "385e4fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 2 embeddings\n",
      "each embedding length 768\n",
      "first 5 dims of first embedding  [-0.00644841  0.01784425  0.00010048 -0.0708848  -0.00857143]\n"
     ]
    }
   ],
   "source": [
    "# embedding chunks with GEMINI \n",
    "\n",
    "#!pip install google-genai langchain\n",
    "\n",
    "import numpy as np\n",
    "from google import genai \n",
    "from google.genai import types\n",
    "\n",
    "client=genai.Client(\n",
    "    api_key=\"AIzaSyDy780rpCWpDX7NKz9oInrjr59dxY0iymE\"\n",
    ")\n",
    "\n",
    "chunks = [\n",
    "    \"LangChain helps build LLM apps. Chunking is step one.\",\n",
    "    \"We split long docs into smaller pieces, often with overlap, to help retrieval.\"\n",
    "]\n",
    "\n",
    "#generate embedding for chunks \n",
    "\n",
    "result=client.models.embed_content(\n",
    "    model='gemini-embedding-001',\n",
    "    contents=chunks,\n",
    "    config=types.EmbedContentConfig(\n",
    "        task_type=\"RETRIEVAL_DOCUMENT\",  # since these are docs to index\n",
    "        output_dimensionality=768\n",
    "    )\n",
    ")\n",
    "\n",
    "embeddings=[np.array(e.values) for e in result.embeddings]\n",
    "\n",
    "print('got' ,len(embeddings),\"embeddings\")\n",
    "\n",
    "print('each embedding length',len(embeddings[0]))\n",
    "\n",
    "print(\"first 5 dims of first embedding \",embeddings[0][:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4167b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb075c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05abcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311502e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
