{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install elasticsearch==8.13.0\n",
    "#!pip install  transformers torch sentence-transformers numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "116e4f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ridhimakapoor/.pyenv/versions/3.10.13/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:204: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v4 of SentenceTransformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "import numpy as np\n",
    "\n",
    "Hugging_face_token=''\n",
    "\n",
    "model=SentenceTransformer(\n",
    "    \"intfloat/multilingual-e5-base\",\n",
    "    use_auth_token=Hugging_face_token\n",
    ")\n",
    "\n",
    "def chunk_text(text,maxLength=400):\n",
    "    words=text.split() \n",
    "    chunks=[]\n",
    "    current_chunk=[]\n",
    "    current_length=0\n",
    "\n",
    "\n",
    "    for word in words:\n",
    "        current_length+=len(word)+1 \n",
    "        if current_length>maxLength:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk=[word]\n",
    "            current_length=len(word)+1 \n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de804bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the md file data \n",
    "with open('parsed-doc/Sukanya Samriddhi Account Scheme 2019 English (1)-with-image-links.md', 'r', encoding='utf-8') as file:\n",
    "     text=file.read() \n",
    "\n",
    "chunks=chunk_text(text)\n",
    "embeddings=[]\n",
    "\n",
    "for chunk in chunks:\n",
    "    embedding=model.encode(f\"passage:{chunk}\",noramlize_embeddings=True)\n",
    "    embedding.append(embedding.tolist())\n",
    "\n",
    "\n",
    "np.save('embedding.npy',embedding)\n",
    "\n",
    "print(f\"number of chunks {len(chunks)}\")\n",
    "print(f\"sample embedding dimmension :{len(embedding[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "\n",
    "es=Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "mappings={\n",
    "    \"properties\":{\n",
    "        \"text\":{\"type\":\"text\"},\n",
    "        \"embedding\":{\n",
    "            \"type\":\"dense_vector\",\n",
    "            \"dims\":768,\n",
    "            \"index\":True,\n",
    "            \"similarity\":\"cosine\"\n",
    "        },\n",
    "        \"language\":{\"type\":\"keyword\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index='markdown_vectors',body={\"mappings\":mappings},ignore=400)\n",
    "\n",
    "print(\"Index 'markdown_vectors' created with 768-dimensional dense_vector field.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727831bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "\n",
    "es=Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "embeddings=np.load('embedding.npy')\n",
    "\n",
    "with open('parsed-doc/Sukanya Samriddhi Account Scheme 2019 English (1)-with-image-links.md', 'r', encoding='utf-8') as file: \n",
    "     text=file.read() \n",
    "     chunks=chunk_text(text)\n",
    "\n",
    "\n",
    "for i, (chunk,embedding) in enumerate(zip(chunks,embeddings)):\n",
    "    doc={\n",
    "         \"text\":chunk,\n",
    "         \"embedding\":embedding.tolist(),\n",
    "         \"language\":\"english\" if all(ord(c)< 128 for c in chunk ) else \"hindi\"\n",
    "    }\n",
    "    es.index(index='markdown_vectors',id=i+1,body=doc)\n",
    "\n",
    "print(f\"indexed{len(chunks)} documents inot 'markdown_vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "from elasticsearch import Elasticsearch \n",
    "\n",
    "model=SentenceTransformer('intfloat/multlingual-e5-base')\n",
    "es=Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "\n",
    "queries=[\n",
    "    \"query: What is the Sukanya Samriddhi Account Scheme?\",\n",
    "    \"query: Who can open a Sukanya Samriddhi account?\",\n",
    "    \"query: What is the interest rate for Sukanya Samriddhi account?\",\n",
    "    \"query: How to withdraw money from Sukanya Samriddhi account?\",\n",
    "    \"query: सुकन्या समृद्धि खाते की ब्याज दर क्या है?\",\n",
    "    \"query: सुकन्या समृद्धि खाता कब बंद किया जा सकता है?\",\n",
    "    \"query: Kavya Kapoor's first post\",\n",
    "    \"query: Haqdarshak agent training details\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    query_embedding=model.encode(q,normalize_embeddings=True).tolist() \n",
    "    search_query={\n",
    "        \"knn\":{\n",
    "            \"filed\":\"embedding\",\n",
    "            \"query_vector\":query_embedding,\n",
    "            \"k\":3,\n",
    "            \"num_candidates\":10\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response=es.search(index='markdown_vectors',body=search_query)\n",
    "\n",
    "    print(f\"\\n searcg results for :{q}\")\n",
    "\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(f\"Score:{hit[\"_source\"]}\")\n",
    "        print(f\"text :{hit[\"_source\"][\"text\"][:120]}...\")\n",
    "        print(\"\\n\\n----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bc987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a18633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4086781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116d6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a56525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899c9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
