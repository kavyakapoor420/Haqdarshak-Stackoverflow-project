{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install elasticsearch transformers torch sentence-transformers numpy \n",
    "# !pip install elasticsearch==8.13.0\n",
    "!pip uninstall elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bb11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    'intfloat/multilingual-e5-base',\n",
    "    use_auth_token=HUGGING_FACE_TOKEN\n",
    ")\n",
    "\n",
    "\n",
    "def chunk_text(text, max_length=400):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1\n",
    "        if current_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "# Read markdown file\n",
    "with open('parsed-doc/Sukanya Samriddhi Account Scheme 2019 English (1)-with-image-links.md', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "    embedding = model.encode(f\"passage: {chunk}\", normalize_embeddings=True)\n",
    "    embeddings.append(embedding.tolist())\n",
    "\n",
    "\n",
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Sample embedding dimension: {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"text\": {\"type\": \"text\"},\n",
    "        \"embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 768,  \n",
    "            \"index\": True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        },\n",
    "        \"language\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "es.indices.create(index=\"markdown_vectors\", body={\"mappings\": mappings}, ignore=400)\n",
    "\n",
    "print(\"Index 'markdown_vectors' created with 768-dimensional dense_vector field.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "\n",
    "embeddings = np.load('embeddings.npy')\n",
    "\n",
    "\n",
    "with open('parsed-doc/Sukanya Samriddhi Account Scheme 2019 English (1)-with-image-links.md', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "chunks = chunk_text(text)  \n",
    "\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    doc = {\n",
    "        \"text\": chunk,\n",
    "        \"embedding\": embedding.tolist(), \n",
    "        \"language\": \"english\" if all(ord(c) < 128 for c in chunk) else \"hindi\"\n",
    "    }\n",
    "    es.index(index=\"markdown_vectors\", id=i+1, body=doc)\n",
    "\n",
    "\n",
    "print(f\"Indexed {len(chunks)} documents into 'markdown_vectors'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-base')\n",
    "es = Elasticsearch([\"http://localhost:9200\"])\n",
    "\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"query: What is the Sukanya Samriddhi Account Scheme?\",\n",
    "    \"query: Who can open a Sukanya Samriddhi account?\",\n",
    "    \"query: What is the interest rate for Sukanya Samriddhi account?\",\n",
    "    \"query: How to withdraw money from Sukanya Samriddhi account?\",\n",
    "    \"query: सुकन्या समृद्धि खाते की ब्याज दर क्या है?\",\n",
    "    \"query: सुकन्या समृद्धि खाता कब बंद किया जा सकता है?\",\n",
    "    \"query: Kavya Kapoor's first post\",\n",
    "    \"query: Haqdarshak agent training details\"\n",
    "]\n",
    "\n",
    "\n",
    "for q in queries:\n",
    "    query_embedding = model.encode(q, normalize_embeddings=True).tolist()\n",
    "    search_query = {\n",
    "        \"knn\": {\n",
    "            \"field\": \"embedding\",\n",
    "            \"query_vector\": query_embedding,\n",
    "            \"k\": 2,\n",
    "            \"num_candidates\": 10\n",
    "        }\n",
    "    }\n",
    "    response = es.search(index=\"markdown_vectors\", body=search_query)\n",
    "    \n",
    "    print(f\"\\nSearch results for: {q}\")\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(f\"Score: {hit['_score']}\")\n",
    "        print(f\"Text: {hit['_source']['text'][:120]}...\")\n",
    "        print(\"\\n\\n---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea197ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2618b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
